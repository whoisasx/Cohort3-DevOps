* go through the notes of previous class before attending the next one.

# week 24.1
1. create a digital ocean account.
    - using ssh key for verification.
    - generate the ssh key on the local machine inside the .ssh folder and using 
      the command 'ssh-keygen'.
2. serve a demo webpage over there.
3. access it by the public domain.

#week 24.2
1.get a GCP account and initiate a virutal machine. 
2.replace nginx with traefic/HAproxy/apache.
3.try deploying a react project.
4.get a domain (namecheap). DONE
5.try ASGs
6.try to do certificate managaement yourself.
7.create a ci/cd pipeline to auto deploy your server from github.
8.forever or pm2(process management)

------------------------------------------------------------------------------------------------

#week 25.2
1. create a monorepo with apps (ws, http, web) and each of them to database and push it to github.
2.  - create 2 servers.
    - add node, nginx to both the servers and also the pnpm.
    - clone the monorepo to both the servers.
    - start all three process http,ws and web.
    - point the name to the specific servers.
    - also deploy the staging websites. 
    - refresh ngnix config.
    - test everything is working.
3. create a ci/cd pipeline.

    {
        whenever we open a terminal anywhere, it automatically source the shell files like (.bashrc,zshrc etc.) but only in the interactive shell. so if we manually ssh into the virtual machine(vm) the shell is interactive and hence the shell files are sourced and we get desired access to the nvm , node , npm and all if they are already installed.
        but in case of github, when github server ssh into the vm , its shell is non-interactive and hence no shell files are sourced and hence no access to anything like node,nvm or npm.
        it will display:[Pseudo-terminal will not be allocated because stdin is not a terminal.]

        even though you manually include the command to source nvm in the deployment file, it still misses the path where all the commands are stored of node and all.
    }

----------------------------------------------------------------------------------------------

#week 26.1
1. install docker Desktop and sign up.
    - run mongo image locally and also remove it.
2. docker image v/s container.
    - image: standalone package contains or encapsulate everything required to run a piece of software. [analogy: consider it as a class.]
    - container: an instance of an image and may be created many more. [analogy: consider an object of a class]
3. understand the concept of port mapping.
4. common docker commands.
    - docker run/kill
    - docker ps/exec
    - docker images
    - docker push
    - docker build [creating your own docker image.]
5. create a docker image locally.
    -build and run a container of the image.
    -push it on docker and remove it from local machine and then run it again to pull it locally.

#week 26.2
1. layers in docker.
    - caching and optimising the cached layers to optimise the build process.
    - always try to use cached layer from previous build if it's not dependent or unchanged.
2. volumes in docker.
3. networks in docker.

--------------------------------------------------------------------------------------------

#week 27.1
1. docker-compose 
2. create a monorepo, and write CI/CD pipeline to deploy it.
Task: monorepo deployment to VM via CI/CD using docker.

-----------------------------------------------------------------------
#week 27.2
Task: Deploying a monorepo to a VM using Docker and CI/CD

1. Use Bun as a package manager for all apps in the monorepo.
   - Understand SSG, CSR, SSR, ISR in Next.js.
   - Clarify the difference between edge and non-edge environments.
   - Note: In Next.js App Router, by default every page is SSG at build time. Next.js decides which pages to statically generate. If a page uses SSG and fetches data from a database, the database connection is needed at build time. This is why you must provide the database URL as a build argument (BUILD ARG) for web apps. For HTTP and WebSocket servers, database access is only needed at runtime, not build time.

2. Create a simple monorepo structure:
   - apps/
     - http-server
     - ws-server
     - web (Next.js)
   - packages/
     - db (Prisma, shared types, etc.)
     - ui (shared UI components)

3. Write a Dockerfile for each app:
   - Use multi-stage builds for smaller images.
   - For web (Next.js):
     - Use `ARG` for `DATABASE_URL` at build time for SSG.
     - Use `ENV` or `env_file` for runtime variables.
   - For http-server and ws-server:
     - Only set `DATABASE_URL` as an environment variable for runtime.

4. Write a `docker-compose.yml`:
   - Define services for each app and the database (e.g., Postgres).
   - Use `env_file` for runtime secrets/configs.
   - Use build `args` for web app build-time variables.
   - Use named volumes for database persistence.
   - Example:
     - `DATABASE_URL=postgresql://postgres:${POSTGRES_PASSWORD}@postgres:5432/postgres`
     - `POSTGRES_PASSWORD` in root `.env` for the database container.

5. Write a CI/CD pipeline:
   - Build and push Docker images to a registry (e.g., Docker Hub).
   - SSH into the VM and pull the latest images.
   - Use `docker-compose` to start/restart the services.
   - Automate the process using GitHub Actions or another CI/CD tool.

6. (Optional) Add health checks, logging, and monitoring for production readiness.

---

This structure ensures clear separation of build-time and runtime configs, secure handling of secrets, and a repeatable deployment process for your monorepo.

---------------------------------------------------------------------------------------------

#week 28.1
Topic: Horizontal and Vertical Scaling

1. Horizontal Scaling (Scaling Out):
    - Increase system capacity by adding more machines/instances.
    - Each instance runs a copy of the app, distributing the load.
    - Common in cloud: add more VMs, containers, pods.
    - Load balancers distribute traffic across instances.

2. Vertical Scaling (Scaling Up):
    - Increase resources (CPU, RAM, etc.) of a single machine.
    - The app runs on a more powerful server.
    - Limited by hardware max capacity; can get expensive.

3. Process Clustering:
    - Run multiple Node.js processes (using cluster module) to use all CPU cores.
    - In development, browsers may cache the process ID and send requests to the same process, so local load balancing may not be fully effective.

4. Stateless vs Stateful Server Autoscaling:
    - Stateless servers: No local user/session data. Any instance can handle any request. Easy to scale horizontally.
    - Stateful servers: Store session/user data locally. Scaling is harder—must route requests to the correct instance or share state (e.g., Redis for sessions).

5. Best Practices:
    - Design apps to be stateless for easier scaling and reliability.
    - Use external storage (databases, caches) for stateful data.
    - Implement health checks and monitoring for autoscaling.

---------------------------------------------------------------------------------------------

#week 28.2
Steps to Create an Auto Scaling Group (ASG) in AWS:

1. Go to the EC2 Dashboard in AWS Console.
2. In the left menu, click on "Auto Scaling Groups".
3. Click "Create Auto Scaling group".
4. Enter a name for your ASG.
5. Select a Launch Template or create a new one (defines AMI, instance type, key pair, security groups, etc.).
6. Choose the VPC and subnets for your instances.
7. Configure group size and scaling policies:
    - Set minimum, desired, and maximum number of instances.
    - Add scaling policies (e.g., target tracking, step scaling, scheduled actions).
8. (Optional) Attach a load balancer (Application/Network Load Balancer) for distributing traffic.
9. Configure notifications and tags as needed.
10. Review all settings and click "Create Auto Scaling group".

Your ASG will now automatically manage EC2 instances based on your scaling policies!

---------------------------------------------------------------------------------------------

#week 29 - Cloud-based IDE Platform with Auto Scaling
Topic: Building a scalable cloud-based VS Code platform using AWS EC2, Auto Scaling Groups, and programmatic instance management.

## Phase 1: Setting Up the Base Cloud IDE Instance

1. **Create an EC2 Instance with Browser-based VS Code**
   - Launch an Ubuntu 22.04 LTS instance (t3.medium or larger recommended)
   - Configure security groups:
     - SSH (22) for administrative access
     - Custom TCP (8080) for VS Code server
     - HTTP/HTTPS (80/443) for web access
   - Install dependencies:
     ```bash
     sudo apt update && sudo apt upgrade -y
     sudo apt install nginx nodejs npm docker.io git curl wget -y
     sudo npm install -g code-server
     ```
   - Configure code-server:
     ```bash
     code-server --bind-addr 0.0.0.0:8080 --auth password
     ```
   - Set up reverse proxy with Nginx for SSL termination
   - Configure firewall and basic security hardening

2. **Create Custom AMI (Amazon Machine Image)**
   - Stop the configured EC2 instance
   - Navigate to EC2 → Instances → Actions → Image and templates → Create image
   - Name: `cloud-ide-base-v1.0`
   - Description: "Base image with VS Code server, Docker, and development tools"
   - Add tags for better organization
   - Test the AMI by launching a new instance from it

## Phase 2: Launch Template and Auto Scaling Configuration

3. **Create Launch Template**
   - EC2 → Launch Templates → Create launch template
   - Template name: `cloud-ide-template`
   - AMI: Select the custom AMI created above
   - Instance type: t3.medium (adjustable based on requirements)
   - Key pair: Select existing or create new
   - Security groups: Create dedicated security group for IDE instances
   - User data script for instance initialization:
     ```bash
     #!/bin/bash
     # Start code-server service
     systemctl start code-server@ubuntu
     systemctl enable code-server@ubuntu
     
     # Configure unique instance identifier
     echo "Instance ID: $(curl -s http://169.254.169.254/latest/meta-data/instance-id)" > /home/ubuntu/instance-info.txt
     
     # Start additional services if needed
     docker system prune -f
     ```
   - Advanced details: Enable detailed monitoring for better scaling decisions

4. **Create Auto Scaling Group (ASG)**
   - Navigate to EC2 → Auto Scaling Groups → Create Auto Scaling group
   - Name: `cloud-ide-asg`
   - Launch template: Select `cloud-ide-template`
   - VPC and subnets: Choose appropriate availability zones
   - Load balancing: Optional (Application Load Balancer for distributing users)
   - Group size configuration:
     - Minimum capacity: 1
     - Desired capacity: 2
     - Maximum capacity: 10
   - Scaling policies:
     - Target tracking scaling (CPU utilization ~70%)
     - Step scaling for rapid demand changes
   - Instance warm-up time: 300 seconds
   - Health check grace period: 300 seconds

## Phase 3: Programmatic ASG Management

5. **Set up IAM User and Permissions**
   - Create IAM user: `asg-controller`
   - Attach custom policy with permissions:
     ```json
     {
       "Version": "2012-10-17",
       "Statement": [
         {
           "Effect": "Allow",
           "Action": [
             "autoscaling:DescribeAutoScalingGroups",
             "autoscaling:UpdateAutoScalingGroup",
             "autoscaling:SetDesiredCapacity",
             "autoscaling:DescribeScalingActivities",
             "ec2:DescribeInstances",
             "ec2:DescribeInstanceStatus"
           ],
           "Resource": "*"
         }
       ]
     }
     ```
   - Generate access key and secret key
   - Store credentials securely in environment variables

6. **Build ASG Management API**
   - Initialize Node.js project:
     ```bash
     npm init -y
     npm install @aws-sdk/client-auto-scaling @aws-sdk/client-ec2 express dotenv cors helmet
     ```
   - Environment variables (`.env`):
     ```
     AWS_ACCESS_KEY_ID=your_access_key
     AWS_SECRET_ACCESS_KEY=your_secret_key
     AWS_REGION=us-east-1
     ASG_NAME=cloud-ide-asg
     PORT=3000
     ```
   - Core functionality:
     - Scale up/down ASG capacity
     - Get current ASG status and instance health
     - List available instances with their public IPs
     - Health checks for instances

## Phase 4: User-Facing API and Instance Allocation

7. **Expose User Endpoints**
   - `POST /api/instance/request` - Request a new development environment
   - `GET /api/instance/status/:userId` - Check instance status for user
   - `POST /api/instance/release/:instanceId` - Release instance when done
   - `GET /api/instances/available` - List available instances
   - `GET /api/asg/metrics` - ASG health and scaling metrics

8. **Additional Features**
   - Instance session management (time-based auto-termination)
   - User authentication and authorization
   - Resource usage monitoring and billing
   - Automatic backups of user workspaces
   - Integration with external storage (S3) for persistent user data

## Phase 5: Monitoring and Optimization

9. **CloudWatch Integration**
   - Set up custom metrics for:
     - Active user sessions per instance
     - Resource utilization (CPU, memory, disk)
     - Instance allocation success/failure rates
   - Configure alarms for scaling decisions
   - Dashboard for real-time monitoring

10. **Cost Optimization**
    - Implement scheduled scaling (scale down during off-hours)
    - Use Spot instances for non-critical workloads
    - Instance hibernation for temporary user absence
    - Regular AMI updates and cleanup of unused resources

## Security Considerations
- Implement network segmentation with private subnets
- Use AWS Systems Manager for secure instance access
- Regular security updates and vulnerability scanning
- Encrypt data at rest and in transit
- Implement proper logging and audit trails

## Testing and Validation
- Load testing for concurrent user scenarios
- Disaster recovery procedures
- Performance benchmarking
- User experience testing across different instance types

This enhanced approach provides a production-ready, scalable cloud IDE platform with proper monitoring, security, and cost optimization.

--------------------------------------------------------------------

#week 30.1 – ECR, ECS & Container Orchestration

**1. AWS CLI Setup**
   - Install or update the AWS CLI to ensure you can run `aws` commands in your terminal.
   - Configure your CLI with `aws configure` (set up access key, secret, region, and output format).

**2. IAM User for ECR**
   - In AWS IAM, create a user with permissions limited to ECR (Amazon Elastic Container Registry) actions.
   - Attach a policy like `AmazonEC2ContainerRegistryFullAccess` or a custom policy with only required permissions.

**3. Access Keys**
   - Generate an Access Key ID and Secret Access Key for the IAM user.
   - Use these credentials to log in via the AWS CLI.

**4. Push Docker Images to ECR**
   - Create a new ECR repository in the AWS Console.
   - Authenticate Docker to your ECR registry:
     ```bash
     aws ecr get-login-password --region <region> | docker login --username AWS --password-stdin <account-id>.dkr.ecr.<region>.amazonaws.com
     ```
   - Build your Docker image and tag it for ECR:
     ```bash
     docker build -t my-app .
     docker tag my-app:latest <account-id>.dkr.ecr.<region>.amazonaws.com/my-app:latest
     ```
   - Push the image:
     ```bash
     docker push <account-id>.dkr.ecr.<region>.amazonaws.com/my-app:latest
     ```

**5. ECS: Elastic Container Service**
   - **Architecture Overview:** Understand ECS components: Clusters, Task Definitions, Services, and Tasks.
   - **Cluster Creation:** Create an ECS cluster (EC2 or Fargate launch type).
   - **Task Definition:** Define your container specs (image, CPU, memory, ports, env vars).
   - **Service Creation:** Deploy a service in your cluster to run and maintain the desired number of tasks.
   - **Autoscaling:** Set up autoscaling policies based on CPU/memory usage or custom CloudWatch metrics.

**Best Practices:**
   - Use IAM roles for ECS tasks for secure access to AWS resources.
   - Store secrets in AWS Secrets Manager or SSM Parameter Store.
   - Enable logging with AWS CloudWatch for observability.

---

#week 30.2 – Monitoring, Logging & New Relic

**1. New Relic Integration**
   - Sign up for a New Relic account.
   - Install the New Relic agent in your application (Node.js, Python, etc.).
   - Configure the agent with your New Relic license key.
   - Use New Relic dashboards to monitor application performance, errors, and throughput.

**2. Winston Logger (Node.js)**
   - Install Winston:
     ```bash
     npm install winston
     ```
   - Set up a logger in your app:
     ```js
     const winston = require('winston');
     const logger = winston.createLogger({
       level: 'info',
       format: winston.format.json(),
       transports: [
         new winston.transports.Console(),
         new winston.transports.File({ filename: 'app.log' })
       ]
     });
     ```
   - Use the logger for structured logging:
     ```js
     logger.info('Server started');
     logger.error('Something went wrong', { error });
     ```

**3. Additional Monitoring & Logging Tips**
   - Forward logs to CloudWatch or a centralized log management system.
   - Set up alerts for error rates, latency, and resource usage.
   - Use distributed tracing (e.g., OpenTelemetry) for end-to-end visibility.

-----------------------------------------------------------------------------------

#week 31 - Prometheus (Node.js Metrics)

1. **Prometheus Metrics in Node.js**
   - Integrated Prometheus metrics collection in a Node.js (Express) app using the `prom-client` library.
   - Exposed a `/metrics` endpoint to allow Prometheus to scrape application metrics.

2. **Custom Metrics Implemented**
   - **Request Counter**: Tracks total number of HTTP requests, labeled by method, route, and status code.
   - **Request Duration Histogram**: Measures duration of HTTP requests in milliseconds, labeled by method, route, and status code.
   - **Active Requests Gauge**: Tracks the number of active HTTP requests being processed.

3. **Metrics Middleware**
   - Middleware increments active requests on each incoming request (except `/metrics`).
   - On response finish, updates request counter, observes request duration, and decrements active requests.

4. **Example Endpoints**
   - `/user` (GET/POST): Simulates user data fetch and creation.
   - `/cpu`: Simulates a CPU-intensive operation.
   - `/metrics`: Exposes all collected metrics in Prometheus format.

5. **Next Steps**
   - Configure Prometheus server to scrape the `/metrics` endpoint.
   - Visualize and alert on metrics (Grafana, etc.) in future sessions.

-----------------------------------------------------------------------------------

#week 32 - Prometheus & Grafana

1. Running Prometheus with your Node app:
   - Starting only the Prometheus container will not work, as there is no process running on port 3000 inside the Prometheus container.
   - Solution: Use Docker Compose to run both Prometheus and your Node.js app on the same Docker network. This allows Prometheus to scrape metrics from your app.
   - Once both containers are running, open http://localhost:9090 to access the Prometheus UI.

2. Querying metrics in Prometheus:
   - Use the Prometheus UI to write and test queries (PromQL) against your collected metrics.
   - Example queries:
     - http_requests_total (total HTTP requests)
     - up (status of monitored targets)

3. Visualizing metrics with Prometheus graphs:
   - Prometheus UI provides basic graphing for quick metric visualization.
   - For more advanced dashboards, use Grafana.

4. Grafana for better visualization:
   - Add a Grafana service to your docker-compose.yml file.
   - Start Grafana and open http://localhost:3001 in your browser.
   - In Grafana, add Prometheus as a data source (URL: http://prometheus:9090 if using Docker Compose).
   - Create dashboards and panels to visualize your Node.js metrics.

5. Alerting in Grafana:
   - Set up alert rules in Grafana to notify you of issues (like high error rates or latency spikes).
   - Configure notification channels (email, Slack, etc.) for alerts.

------------------------------------------------------------------------------------------

#week 33.1 - Serverless

1. **Introduction to Serverless Backends**
   - Overview of serverless architecture and its benefits (scalability, cost, no server management).
   - Popular serverless providers: AWS Lambda, Google Cloud Functions, Azure Functions, Cloudflare Workers, Vercel, Netlify.

2. **Cloudflare Workers Setup**
   - Install the Wrangler CLI:  
     `npm install -g wrangler`
   - Authenticate and configure your Cloudflare account.
   - Initialize a new worker project:  
     `wrangler init my-worker`
   - Project structure and configuration files explained.

3. **How Cloudflare Workers Work**
   - Edge computing: Workers run at Cloudflare’s edge locations, close to users.
   - Event-driven: Each request triggers a lightweight JavaScript/TypeScript function.
   - No cold starts, fast response times, limited runtime environment.

4. **Initializing and Deploying a Worker**
   - Write a simple handler (e.g., return "Hello World" or echo request).
   - Test locally with `wrangler dev`.
   - Deploy to Cloudflare with `wrangler publish`.
   - Access your worker at the provided URL.

5. **Hono as an Express Alternative for Backends**
   - Introduction to Hono: a fast, lightweight web framework for Cloudflare Workers and other edge runtimes.
   - Install Hono:  
     `npm install hono`
   - Basic usage: define routes, handle requests, return responses.

6. **Middlewares in Hono**
   - How to use and write middlewares (e.g., logging, authentication, CORS).
   - Example: Add a middleware to log incoming requests.

7. **Database Connections in Serverless**
   - The problem: Traditional database connections (e.g., with Prisma) don’t work well in serverless due to connection limits and statelessness.
   - Solution: Use connection pooling and serverless-optimized database adapters.
   - **Prisma Accelerate:**  
     - What it is: A service to enable efficient connection pooling for Prisma in serverless environments.
     - How to set up:  
       - Enable Prisma Accelerate in your project.
       - Update your Prisma connection string to use Accelerate.
     - Benefits: Reduced connection overhead, better scalability.

8. **Building a Basic Todo CRUD App with Hono**
   - Set up routes for creating, reading, updating, and deleting todos.
   - Use a serverless-compatible database (e.g., D1, PlanetScale, Neon, or KV).
   - Deploy and test the CRUD API on Cloudflare Workers.

------------------------------------------------------------------------------------------